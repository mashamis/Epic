{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEWqgkD692dZ"
      },
      "source": [
        "# Exploring the rank of trained Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K3VP30ED-B_g"
      },
      "source": [
        "In this Final assignment, you're going to explore trained neural networks, and study the rank of its matrices.\n",
        "\n",
        "**Reminder**: The rank is the number of independent columns of the matrix. If a matrix $A \\in \\mathbb{R}^{n\\times m}$  has rank $k$, then $A$ can be approximated by\n",
        "\n",
        "$$A \\approx B \\cdot C$$\n",
        "\n",
        "where $B \\in \\mathbb{R}^{n\\times k}$ and $C \\in \\mathbb{R}^{k\\times m}$.\n",
        "\n",
        "You can find the rank of matrix $A$ by performing Gaussian elimination and counting the number of pivots. This can be done in few lines of `numpy` code.\n",
        "\n",
        "**References**:\n",
        "- https://arxiv.org/pdf/1804.08838\n",
        "- https://arxiv.org/pdf/2209.13569\n",
        "- https://arxiv.org/pdf/2012.13255\n",
        "\n",
        "Note: The references above are not needed to complete this notebook, but reading them might give you additional insights."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jUKDdnCbMLSC"
      },
      "source": [
        "## Important\n",
        "\n",
        "1. For all the training done, make sure to plot things like the loss values and accuracy on each epoch.\n",
        "\n",
        "    - You can either use tensorboard or just make a static matplotlib plot.\n",
        "    \n",
        "2. Don't add biases to the layers in the network, not important for this notebook.\n",
        "3. No need to use Dropout or BatchNorm on the network.\n",
        "4. Remember to use GPUs during the training.\n",
        "5. Always test your hypothesis on both training and testing sets, you might get a surprising result sometimes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QlFG5uMx_uPR"
      },
      "source": [
        "## Task 1: Downloading MNIST and Dataloaders\n",
        "\n",
        "Download the MNIST dataset and split into training and testing, and create dataloaders.\n",
        "\n",
        "Link: https://pytorch.org/vision/stable/generated/torchvision.datasets.MNIST.html"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jU4hXFXr_xJT"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "import torch.utils.data as data\n",
        "\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "from sklearn import metrics\n",
        "from sklearn import decomposition\n",
        "from sklearn import manifold\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXsytH1ZtjST",
        "outputId": "d01e988f-f98c-4cd2-d1ad-38b675056e86"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 22409593.05it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 1738157.81it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 3009684.76it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Failed to download (trying next):\n",
            "HTTP Error 403: Forbidden\n",
            "\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading https://ossci-datasets.s3.amazonaws.com/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 4061946.43it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "#loading train and test data\n",
        "train_set = datasets.MNIST(root='./data', train=True, download=True)\n",
        "test_set = datasets.MNIST(root='./data', train=False, download=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKyAv2V6-l0x",
        "outputId": "5b1a71f8-35d7-4022-cc64-77903f14c3a1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Calculated mean: 0.13066047430038452\n",
            "Calculated std: 0.30810779333114624\n"
          ]
        }
      ],
      "source": [
        "mean = train_set.data.float().mean() / 255\n",
        "std = train_set.data.float().std() / 255\n",
        "\n",
        "print(f'Calculated mean: {mean}')\n",
        "print(f'Calculated std: {std}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "89x8iChG_8tS"
      },
      "outputs": [],
      "source": [
        "#preprocessing our data and transforming it to a suitable format\n",
        "train_transforms = transforms.Compose([\n",
        "                            transforms.RandomRotation(5, fill=(0,)),\n",
        "                            transforms.RandomCrop(28, padding = 2),\n",
        "                            transforms.ToTensor(),\n",
        "                            transforms.Normalize(mean = [mean], std = [std])\n",
        "                                      ])\n",
        "\n",
        "test_transforms = transforms.Compose([\n",
        "                           transforms.ToTensor(),\n",
        "                           transforms.Normalize(mean = [mean], std = [std])\n",
        "                                     ])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sd2BHyxjAG21"
      },
      "outputs": [],
      "source": [
        "#loading data again with the following transforms\n",
        "train_data = datasets.MNIST(root = './data',\n",
        "                            train = True,\n",
        "                            download = True,\n",
        "                            transform = train_transforms)\n",
        "\n",
        "test_data = datasets.MNIST(root = './data',\n",
        "                           train = False,\n",
        "                           download = True,\n",
        "                           transform = test_transforms)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fftXIX59W8PJ"
      },
      "outputs": [],
      "source": [
        "VALID_RATIO = 0.9\n",
        "\n",
        "n_train_examples = int(len(train_data) * VALID_RATIO)\n",
        "n_valid_examples = len(train_data) - n_train_examples\n",
        "\n",
        "train_data, valid_data = data.random_split(train_data,\n",
        "                                  [n_train_examples, n_valid_examples])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0LZkWYcuXIOm"
      },
      "outputs": [],
      "source": [
        "valid_data = copy.deepcopy(valid_data)\n",
        "valid_data.dataset.transform = test_transforms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rXZ-sDh72RYo"
      },
      "outputs": [],
      "source": [
        "#creating data loaders\n",
        "batch_size = 64\n",
        "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
        "valid_iterator = data.DataLoader(valid_data, batch_size = batch_size, shuffle=False)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLAtp2bM-W5z"
      },
      "source": [
        "## Task 2: Train a neural network\n",
        "\n",
        "Build a simple Multi-layered Perceptron with ReLU activations, and train it on MNIST until achieving 95% accuracy or higher.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x8ikfbxo914-"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 250)\n",
        "        self.hidden_fc = nn.Linear(250, 100)\n",
        "        self.output_fc = nn.Linear(100, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "        h_1 = F.relu(self.input_fc(x))\n",
        "        h_2 = F.relu(self.hidden_fc(h_1))\n",
        "        y_pred = self.output_fc(h_2)\n",
        "        return y_pred, h_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hwXisjIuEJtr"
      },
      "outputs": [],
      "source": [
        "#defining the model\n",
        "INPUT_DIM = 28 * 28\n",
        "OUTPUT_DIM = 10\n",
        "\n",
        "model = MLP(INPUT_DIM, OUTPUT_DIM)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crBpm5pXAS_v"
      },
      "outputs": [],
      "source": [
        "# defining the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "criterion = criterion.to(device)\n",
        "\n",
        "#calculating accuracy\n",
        "def calculate_accuracy(y_pred, y):\n",
        "    top_pred = y_pred.argmax(1, keepdim = True)\n",
        "    correct = top_pred.eq(y.view_as(top_pred)).sum()\n",
        "    acc = correct.float() / y.shape[0]\n",
        "    return acc\n",
        "\n",
        "#defining training loop\n",
        "def train(model, iterator, optimizer, criterion, device):\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    for (x, y) in iterator:\n",
        "\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        y_pred, _ = model(x)\n",
        "        loss = criterion(y_pred, y)\n",
        "        acc = calculate_accuracy(y_pred, y)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "        epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
        "\n",
        "#defining evaluating loop\n",
        "def evaluate(model, iterator, criterion, device):\n",
        "\n",
        "    epoch_loss = 0\n",
        "    epoch_acc = 0\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for (x, y) in iterator:\n",
        "            x = x.to(device)\n",
        "            y = y.to(device)\n",
        "            y_pred, _ = model(x)\n",
        "            loss = criterion(y_pred, y)\n",
        "            acc = calculate_accuracy(y_pred, y)\n",
        "            epoch_loss += loss.item()\n",
        "            epoch_acc += acc.item()\n",
        "\n",
        "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NaTsiHlvIPxj",
        "outputId": "8be101fb-343c-48d1-c668-8d59748a9d13"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.073 | Train Acc: 97.76%\n",
            "\t Val. Loss: 0.051 |  Val. Acc: 98.38%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.070 | Train Acc: 97.75%\n",
            "\t Val. Loss: 0.078 |  Val. Acc: 97.62%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.068 | Train Acc: 97.88%\n",
            "\t Val. Loss: 0.055 |  Val. Acc: 98.32%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.065 | Train Acc: 97.94%\n",
            "\t Val. Loss: 0.067 |  Val. Acc: 97.86%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.064 | Train Acc: 97.93%\n",
            "\t Val. Loss: 0.069 |  Val. Acc: 98.02%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.061 | Train Acc: 98.14%\n",
            "\t Val. Loss: 0.062 |  Val. Acc: 98.18%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.061 | Train Acc: 98.08%\n",
            "\t Val. Loss: 0.056 |  Val. Acc: 98.40%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.060 | Train Acc: 98.12%\n",
            "\t Val. Loss: 0.052 |  Val. Acc: 98.48%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.061 | Train Acc: 98.08%\n",
            "\t Val. Loss: 0.054 |  Val. Acc: 98.54%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.057 | Train Acc: 98.28%\n",
            "\t Val. Loss: 0.057 |  Val. Acc: 98.45%\n"
          ]
        }
      ],
      "source": [
        "#train it on MNIST until achieving 95% accuracy or higher\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eM59rQcqLCeo",
        "outputId": "d72a94a0-2943-4c04-a4ce-7d01c71a699f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.050 | Test Acc: 98.42%\n"
          ]
        }
      ],
      "source": [
        "#evaluating the model on the test set\n",
        "test_loss, test_acc = evaluate(model, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7q1K0drgAIPX"
      },
      "source": [
        "## Task 3: Analyze the rank of the matrices in this network\n",
        "\n",
        "Perform experiments and answer the following questions:\n",
        "- What's the average rank of the matrices on all layers?\n",
        "- How does the rank increase as we go to deeper layers?\n",
        "- Try the same MLP, but change the activation function to others ($\\tanh, \\sigma, \\dots$). Do the answers change?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PtL3NVMOL6yU",
        "outputId": "41de7c20-0c3b-43ed-d49e-0597b3b62bfe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: input_fc.weight, Rank: 250\n",
            "Layer: hidden_fc.weight, Rank: 100\n",
            "Layer: output_fc.weight, Rank: 10\n"
          ]
        }
      ],
      "source": [
        "#function to calculate average rank and rank per each layer\n",
        "weight_matrices = [model.input_fc.weight, model.hidden_fc.weight, model.output_fc.weight]\n",
        "def calculate_ranks(model):\n",
        "    ranks = []\n",
        "    for name, param in model.named_parameters():\n",
        "        if 'weight' in name:\n",
        "            weight_matrix = param.data.numpy()\n",
        "            rank = np.linalg.matrix_rank(weight_matrix)\n",
        "            ranks.append(rank)\n",
        "            print(f\"Layer: {name}, Rank: {rank}\")\n",
        "    return ranks\n",
        "\n",
        "ranks = calculate_ranks(model)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "chBNamtb50LU",
        "outputId": "48da39a1-540f-4bba-8203-93c728c27358"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Rank: 120.0\n"
          ]
        }
      ],
      "source": [
        "#calculating evarsge rank of the matrix\n",
        "average_rank = np.mean(ranks)\n",
        "print(f\"Average Rank: {average_rank}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb4nQje4Mf8t"
      },
      "source": [
        "As we can see, as we go deeper into the network the rank decreases.\n",
        "\n",
        "It is also noticeable than ranks are equal to the dimensions of the following layers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xhN8dGucNRLg"
      },
      "outputs": [],
      "source": [
        "#same MLP but with the tanh activation function to others\n",
        "class MLP_tanh(nn.Module):\n",
        "    def __init__(self, input_dim, output_dim):\n",
        "        super().__init__()\n",
        "\n",
        "        self.input_fc = nn.Linear(input_dim, 250)\n",
        "        self.hidden_fc = nn.Linear(250, 100)\n",
        "        self.output_fc = nn.Linear(100, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        batch_size = x.shape[0]\n",
        "        x = x.view(batch_size, -1)\n",
        "        h_1 = F.tanh(self.input_fc(x))\n",
        "        h_2 = F.tanh(self.hidden_fc(h_1))\n",
        "        y_pred = self.output_fc(h_2)\n",
        "        return y_pred, h_2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cQGkNscmN3Tl",
        "outputId": "df3a5f78-4853-42b6-ed83-4b774a7b35c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 01\n",
            "\tTrain Loss: 0.454 | Train Acc: 86.37%\n",
            "\t Val. Loss: 0.155 |  Val. Acc: 95.63%\n",
            "Epoch: 02\n",
            "\tTrain Loss: 0.184 | Train Acc: 94.33%\n",
            "\t Val. Loss: 0.106 |  Val. Acc: 96.55%\n",
            "Epoch: 03\n",
            "\tTrain Loss: 0.151 | Train Acc: 95.25%\n",
            "\t Val. Loss: 0.106 |  Val. Acc: 96.66%\n",
            "Epoch: 04\n",
            "\tTrain Loss: 0.135 | Train Acc: 95.72%\n",
            "\t Val. Loss: 0.098 |  Val. Acc: 96.97%\n",
            "Epoch: 05\n",
            "\tTrain Loss: 0.126 | Train Acc: 96.00%\n",
            "\t Val. Loss: 0.094 |  Val. Acc: 97.06%\n",
            "Epoch: 06\n",
            "\tTrain Loss: 0.118 | Train Acc: 96.26%\n",
            "\t Val. Loss: 0.080 |  Val. Acc: 97.61%\n",
            "Epoch: 07\n",
            "\tTrain Loss: 0.113 | Train Acc: 96.43%\n",
            "\t Val. Loss: 0.071 |  Val. Acc: 97.60%\n",
            "Epoch: 08\n",
            "\tTrain Loss: 0.106 | Train Acc: 96.71%\n",
            "\t Val. Loss: 0.075 |  Val. Acc: 97.82%\n",
            "Epoch: 09\n",
            "\tTrain Loss: 0.104 | Train Acc: 96.78%\n",
            "\t Val. Loss: 0.071 |  Val. Acc: 97.79%\n",
            "Epoch: 10\n",
            "\tTrain Loss: 0.105 | Train Acc: 96.66%\n",
            "\t Val. Loss: 0.086 |  Val. Acc: 96.88%\n"
          ]
        }
      ],
      "source": [
        "model_1 = MLP_tanh(INPUT_DIM, OUTPUT_DIM)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_1 = optim.Adam(model_1.parameters())\n",
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    train_loss, train_acc = train(model_1, train_loader, optimizer_1, criterion, device)\n",
        "    valid_loss, valid_acc = evaluate(model_1, valid_iterator, criterion, device)\n",
        "\n",
        "    print(f'Epoch: {epoch+1:02}')\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nze4TNyQOMV5",
        "outputId": "0222b838-20a4-4fd0-de4a-8283573542f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.077 | Test Acc: 97.40%\n"
          ]
        }
      ],
      "source": [
        "#evaluating the model on the test set\n",
        "test_loss, test_acc = evaluate(model_1, test_loader, criterion, device)\n",
        "print(f'Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i7M2A7iAOV1P",
        "outputId": "dc847bc7-0964-41b3-de11-51795b68cea3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: input_fc.weight, Rank: 250\n",
            "Layer: hidden_fc.weight, Rank: 100\n",
            "Layer: output_fc.weight, Rank: 10\n",
            "Average Rank: 120.0\n"
          ]
        }
      ],
      "source": [
        "ranks = calculate_ranks(model_1)\n",
        "average_rank = np.mean(ranks)\n",
        "print(f\"Average Rank: {average_rank}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JftrstL3Pr-6"
      },
      "source": [
        "The accuracy remained almost the same, ranks themselves didn't change at all."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Htp37DfHCQKL"
      },
      "source": [
        "## Task 4: Overfit by scaling the MLP\n",
        "\n",
        "1. Create a bigger network and train it on MNIST, to the point of overfitting.\n",
        "2. Now check the rank of the matrices in the network, and answer the same questions."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#creating a bigger MLP\n",
        "class LargeMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LargeMLP, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 64)\n",
        "        self.fc5 = nn.Linear(64, 10)  # output layer\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = torch.relu(self.fc4(x))\n",
        "        x = self.fc5(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "tOYPL7SGeEzF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying a larger model\n",
        "large_model = LargeMLP()\n",
        "large_criterion = nn.CrossEntropyLoss()\n",
        "large_optimizer = optim.Adam(large_model.parameters(), lr=0.001)\n",
        "\n",
        "large_model.to(device)\n",
        "\n",
        "large_train_losses = []\n",
        "large_train_accuracies = []\n",
        "num_epochs = 40\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    large_model.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        large_optimizer.zero_grad()\n",
        "        outputs = large_model(inputs)\n",
        "        loss = large_criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        large_optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    large_train_losses.append(train_loss)\n",
        "    large_train_accuracies.append(train_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}],'f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pU0932oWeKJR",
        "outputId": "5a57a959-0bc7-45c9-be6b-595756735689"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/40],\tTrain Loss: 0.403 | Train Acc: 87.09%\n",
            "Epoch [2/40],\tTrain Loss: 0.175 | Train Acc: 94.61%\n",
            "Epoch [3/40],\tTrain Loss: 0.136 | Train Acc: 95.77%\n",
            "Epoch [4/40],\tTrain Loss: 0.115 | Train Acc: 96.50%\n",
            "Epoch [5/40],\tTrain Loss: 0.109 | Train Acc: 96.57%\n",
            "Epoch [6/40],\tTrain Loss: 0.102 | Train Acc: 96.84%\n",
            "Epoch [7/40],\tTrain Loss: 0.092 | Train Acc: 97.14%\n",
            "Epoch [8/40],\tTrain Loss: 0.086 | Train Acc: 97.39%\n",
            "Epoch [9/40],\tTrain Loss: 0.084 | Train Acc: 97.44%\n",
            "Epoch [10/40],\tTrain Loss: 0.078 | Train Acc: 97.62%\n",
            "Epoch [11/40],\tTrain Loss: 0.076 | Train Acc: 97.69%\n",
            "Epoch [12/40],\tTrain Loss: 0.067 | Train Acc: 97.96%\n",
            "Epoch [13/40],\tTrain Loss: 0.067 | Train Acc: 97.93%\n",
            "Epoch [14/40],\tTrain Loss: 0.067 | Train Acc: 97.99%\n",
            "Epoch [15/40],\tTrain Loss: 0.065 | Train Acc: 98.03%\n",
            "Epoch [16/40],\tTrain Loss: 0.062 | Train Acc: 98.11%\n",
            "Epoch [17/40],\tTrain Loss: 0.060 | Train Acc: 98.18%\n",
            "Epoch [18/40],\tTrain Loss: 0.060 | Train Acc: 98.20%\n",
            "Epoch [19/40],\tTrain Loss: 0.056 | Train Acc: 98.32%\n",
            "Epoch [20/40],\tTrain Loss: 0.055 | Train Acc: 98.31%\n",
            "Epoch [21/40],\tTrain Loss: 0.053 | Train Acc: 98.38%\n",
            "Epoch [22/40],\tTrain Loss: 0.053 | Train Acc: 98.38%\n",
            "Epoch [23/40],\tTrain Loss: 0.050 | Train Acc: 98.46%\n",
            "Epoch [24/40],\tTrain Loss: 0.050 | Train Acc: 98.49%\n",
            "Epoch [25/40],\tTrain Loss: 0.049 | Train Acc: 98.50%\n",
            "Epoch [26/40],\tTrain Loss: 0.049 | Train Acc: 98.53%\n",
            "Epoch [27/40],\tTrain Loss: 0.049 | Train Acc: 98.56%\n",
            "Epoch [28/40],\tTrain Loss: 0.047 | Train Acc: 98.55%\n",
            "Epoch [29/40],\tTrain Loss: 0.047 | Train Acc: 98.54%\n",
            "Epoch [30/40],\tTrain Loss: 0.043 | Train Acc: 98.65%\n",
            "Epoch [31/40],\tTrain Loss: 0.045 | Train Acc: 98.63%\n",
            "Epoch [32/40],\tTrain Loss: 0.044 | Train Acc: 98.57%\n",
            "Epoch [33/40],\tTrain Loss: 0.044 | Train Acc: 98.64%\n",
            "Epoch [34/40],\tTrain Loss: 0.042 | Train Acc: 98.74%\n",
            "Epoch [35/40],\tTrain Loss: 0.043 | Train Acc: 98.70%\n",
            "Epoch [36/40],\tTrain Loss: 0.040 | Train Acc: 98.75%\n",
            "Epoch [37/40],\tTrain Loss: 0.041 | Train Acc: 98.74%\n",
            "Epoch [38/40],\tTrain Loss: 0.043 | Train Acc: 98.71%\n",
            "Epoch [39/40],\tTrain Loss: 0.039 | Train Acc: 98.77%\n",
            "Epoch [40/40],\tTrain Loss: 0.038 | Train Acc: 98.84%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jb35EdO1hqhW",
        "outputId": "28c86e22-8a91-4db7-b943-e51d8e1d28bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Layer: fc1.weight, Rank: 512\n",
            "Layer: fc2.weight, Rank: 256\n",
            "Layer: fc3.weight, Rank: 128\n",
            "Layer: fc4.weight, Rank: 64\n",
            "Layer: fc5.weight, Rank: 10\n",
            "Average Rank: 194.0\n"
          ]
        }
      ],
      "source": [
        "ranks = calculate_ranks(large_model)\n",
        "average_rank = np.mean(ranks)\n",
        "print(f\"Average Rank: {average_rank}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sl469SP-i-MT"
      },
      "source": [
        "As we can see, even though the model is overfitted, all ranks are still equal to the dimensions of the layers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-jSb8aKmMDcB"
      },
      "source": [
        "## Task 5: Approximate low-rank\n",
        "\n",
        "From some of the references given at the beginning, you can realize that trained neural networks have intrinsically low dimensionality (meaning low-rank matrices).\n",
        "\n",
        "In this task, take the overparametrized network already trained from the TASK4 and try to approximate each layer's matrix with a product of two other low-rank matrices?\n",
        "\n",
        "This means, if a layer has a matrix $A \\in\\mathbb{R}^{n\\times m}$, then try to find two matrices $B \\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$ so that $\\lvert {A - B\\cdot C}\\rvert $ is minimized, where $\\lvert x\\rvert$ means the Frobenius norm. You can use a different norm, if you think it makes sense. In order to learn $B$ and $C$, you can do gradient descent-like algorithms, where you alternate between updating $B$ and $C$ on each optimization step.\n",
        "\n",
        "**Ablate**:\n",
        "Try different values for $r$ and analyze how good your approximation is (for e.g, by taking average Frobenius norm across all layers) as you increase $r$. Make a plot with that.\n",
        "\n",
        "Conclude what is the effective rank $r$: the smallest rank such that the approximation of that rank is good enough (meaning the Frobenius norm is smaller than some threshold chosen by you)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X4p4_ZdQducM"
      },
      "outputs": [],
      "source": [
        "# Function to perform low-rank matrix factorization\n",
        "def low_rank_approximation(weight_matrix, rank, num_iterations=1000, lr=0.01, verbose=True):\n",
        "    m, n = weight_matrix.shape\n",
        "    B = torch.randn(m, rank, requires_grad=True, device=device)\n",
        "    C = torch.randn(rank, n, requires_grad=True, device=device)\n",
        "\n",
        "    optimizer = optim.Adam([B, C], lr=lr)\n",
        "\n",
        "    frobenius_norms = []\n",
        "\n",
        "    for i in range(num_iterations):\n",
        "        optimizer.zero_grad()\n",
        "        approx_matrix = torch.matmul(B, C)\n",
        "        loss = torch.norm(weight_matrix - approx_matrix, 'fro') #frobenius norms\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if verbose and (i+1) % 100 == 0:\n",
        "            print(f'Epoch [{i+1}/{num_iterations}], Loss: {loss.item():.4f}')\n",
        "\n",
        "        frobenius_norms.append(loss.item())\n",
        "\n",
        "    #average value of the frobenius norm the layer\n",
        "    final_layer_loss = frobenius_norms[-1]\n",
        "    print('Final loss on current layer: ', final_layer_loss)\n",
        "    print()\n",
        "\n",
        "    return B.detach(), C.detach(), frobenius_norms, final_layer_loss\n",
        "\n",
        "def apply_low_rank_approximation(model, rank_values):\n",
        "    for i, rank in enumerate(rank_values):\n",
        "        print('Norm values for rank ', rank, ':')\n",
        "        final_losses = []\n",
        "        for j, (name, param) in enumerate(model.named_parameters()):\n",
        "\n",
        "            if 'weight' in name:\n",
        "                weight_matrix = param.detach().cpu().numpy()\n",
        "                weight_tensor = torch.tensor(weight_matrix, device=device)\n",
        "\n",
        "                # low-rank approximation\n",
        "                B, C, frobenius_norms, final_layer_loss = low_rank_approximation(weight_tensor, rank)\n",
        "                final_losses.append(final_layer_loss)\n",
        "                # updating weight matrix with low-rank approximation\n",
        "                with torch.no_grad():\n",
        "                    new_weight_matrix = torch.matmul(B, C).cpu().numpy()\n",
        "                    param.copy_(torch.tensor(new_weight_matrix, device=device))\n",
        "        mean = np.mean(final_losses)\n",
        "        print('Mean frobenius value for current rank: ', mean)\n",
        "        print()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DPqL6lyKuKkV",
        "outputId": "e69322ea-f848-483a-8483-534716073f9b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Norm values for rank  5 :\n",
            "Epoch [100/1000], Loss: 271.8906\n",
            "Epoch [200/1000], Loss: 28.5072\n",
            "Epoch [300/1000], Loss: 5.0118\n",
            "Epoch [400/1000], Loss: 4.8898\n",
            "Epoch [500/1000], Loss: 4.8747\n",
            "Epoch [600/1000], Loss: 4.8588\n",
            "Epoch [700/1000], Loss: 4.8143\n",
            "Epoch [800/1000], Loss: 4.7573\n",
            "Epoch [900/1000], Loss: 4.6312\n",
            "Epoch [1000/1000], Loss: 4.4148\n",
            "Final loss on current layer:  4.414775371551514\n",
            "\n",
            "Epoch [100/1000], Loss: 125.7987\n",
            "Epoch [200/1000], Loss: 15.8924\n",
            "Epoch [300/1000], Loss: 13.9436\n",
            "Epoch [400/1000], Loss: 10.8530\n",
            "Epoch [500/1000], Loss: 9.4193\n",
            "Epoch [600/1000], Loss: 8.0707\n",
            "Epoch [700/1000], Loss: 6.3410\n",
            "Epoch [800/1000], Loss: 4.6433\n",
            "Epoch [900/1000], Loss: 3.3837\n",
            "Epoch [1000/1000], Loss: 2.5697\n",
            "Final loss on current layer:  2.569650650024414\n",
            "\n",
            "Epoch [100/1000], Loss: 66.5053\n",
            "Epoch [200/1000], Loss: 10.1900\n",
            "Epoch [300/1000], Loss: 5.6859\n",
            "Epoch [400/1000], Loss: 3.9254\n",
            "Epoch [500/1000], Loss: 2.3704\n",
            "Epoch [600/1000], Loss: 1.0769\n",
            "Epoch [700/1000], Loss: 0.7306\n",
            "Epoch [800/1000], Loss: 0.6967\n",
            "Epoch [900/1000], Loss: 0.6967\n",
            "Epoch [1000/1000], Loss: 0.6951\n",
            "Final loss on current layer:  0.6950876712799072\n",
            "\n",
            "Epoch [100/1000], Loss: 40.3488\n",
            "Epoch [200/1000], Loss: 4.3392\n",
            "Epoch [300/1000], Loss: 1.9138\n",
            "Epoch [400/1000], Loss: 0.4353\n",
            "Epoch [500/1000], Loss: 0.4362\n",
            "Epoch [600/1000], Loss: 0.4402\n",
            "Epoch [700/1000], Loss: 0.4414\n",
            "Epoch [800/1000], Loss: 0.4457\n",
            "Epoch [900/1000], Loss: 0.4396\n",
            "Epoch [1000/1000], Loss: 0.4409\n",
            "Final loss on current layer:  0.4408550560474396\n",
            "\n",
            "Epoch [100/1000], Loss: 9.4846\n",
            "Epoch [200/1000], Loss: 1.8981\n",
            "Epoch [300/1000], Loss: 0.9329\n",
            "Epoch [400/1000], Loss: 0.5334\n",
            "Epoch [500/1000], Loss: 0.2897\n",
            "Epoch [600/1000], Loss: 0.1680\n",
            "Epoch [700/1000], Loss: 0.1443\n",
            "Epoch [800/1000], Loss: 0.1428\n",
            "Epoch [900/1000], Loss: 0.1417\n",
            "Epoch [1000/1000], Loss: 0.1418\n",
            "Final loss on current layer:  0.14183387160301208\n",
            "\n",
            "Mean frobenius value for current rank:  1.6524405241012574\n",
            "\n",
            "Norm values for rank  10 :\n",
            "Epoch [100/1000], Loss: 351.3206\n",
            "Epoch [200/1000], Loss: 29.4589\n",
            "Epoch [300/1000], Loss: 2.3887\n",
            "Epoch [400/1000], Loss: 2.3975\n",
            "Epoch [500/1000], Loss: 2.4101\n",
            "Epoch [600/1000], Loss: 2.3206\n",
            "Epoch [700/1000], Loss: 2.3130\n",
            "Epoch [800/1000], Loss: 2.3117\n",
            "Epoch [900/1000], Loss: 2.3325\n",
            "Epoch [1000/1000], Loss: 2.3164\n",
            "Final loss on current layer:  2.316427707672119\n",
            "\n",
            "Epoch [100/1000], Loss: 185.1956\n",
            "Epoch [200/1000], Loss: 14.7209\n",
            "Epoch [300/1000], Loss: 9.0922\n",
            "Epoch [400/1000], Loss: 3.8486\n",
            "Epoch [500/1000], Loss: 1.9266\n",
            "Epoch [600/1000], Loss: 1.8594\n",
            "Epoch [700/1000], Loss: 1.8476\n",
            "Epoch [800/1000], Loss: 1.8624\n",
            "Epoch [900/1000], Loss: 1.8368\n",
            "Epoch [1000/1000], Loss: 1.8419\n",
            "Final loss on current layer:  1.8419406414031982\n",
            "\n",
            "Epoch [100/1000], Loss: 80.6918\n",
            "Epoch [200/1000], Loss: 9.1101\n",
            "Epoch [300/1000], Loss: 4.1504\n",
            "Epoch [400/1000], Loss: 1.6145\n",
            "Epoch [500/1000], Loss: 0.8316\n",
            "Epoch [600/1000], Loss: 0.5987\n",
            "Epoch [700/1000], Loss: 0.4961\n",
            "Epoch [800/1000], Loss: 0.4471\n",
            "Epoch [900/1000], Loss: 0.4203\n",
            "Epoch [1000/1000], Loss: 0.4029\n",
            "Final loss on current layer:  0.4028806984424591\n",
            "\n",
            "Epoch [100/1000], Loss: 33.3462\n",
            "Epoch [200/1000], Loss: 2.2024\n",
            "Epoch [300/1000], Loss: 0.2870\n",
            "Epoch [400/1000], Loss: 0.2390\n",
            "Epoch [500/1000], Loss: 0.2202\n",
            "Epoch [600/1000], Loss: 0.2077\n",
            "Epoch [700/1000], Loss: 0.2064\n",
            "Epoch [800/1000], Loss: 0.2009\n",
            "Epoch [900/1000], Loss: 0.1969\n",
            "Epoch [1000/1000], Loss: 0.1982\n",
            "Final loss on current layer:  0.19815267622470856\n",
            "\n",
            "Epoch [100/1000], Loss: 3.2320\n",
            "Epoch [200/1000], Loss: 1.1280\n",
            "Epoch [300/1000], Loss: 0.2802\n",
            "Epoch [400/1000], Loss: 0.1171\n",
            "Epoch [500/1000], Loss: 0.0776\n",
            "Epoch [600/1000], Loss: 0.0695\n",
            "Epoch [700/1000], Loss: 0.0636\n",
            "Epoch [800/1000], Loss: 0.0608\n",
            "Epoch [900/1000], Loss: 0.0589\n",
            "Epoch [1000/1000], Loss: 0.0573\n",
            "Final loss on current layer:  0.05726444721221924\n",
            "\n",
            "Mean frobenius value for current rank:  0.9633332341909409\n",
            "\n",
            "Norm values for rank  20 :\n",
            "Epoch [100/1000], Loss: 455.1038\n",
            "Epoch [200/1000], Loss: 22.7111\n",
            "Epoch [300/1000], Loss: 3.4097\n",
            "Epoch [400/1000], Loss: 2.3646\n",
            "Epoch [500/1000], Loss: 2.1420\n",
            "Epoch [600/1000], Loss: 1.9784\n",
            "Epoch [700/1000], Loss: 1.9346\n",
            "Epoch [800/1000], Loss: 1.8871\n",
            "Epoch [900/1000], Loss: 1.8512\n",
            "Epoch [1000/1000], Loss: 1.8256\n",
            "Final loss on current layer:  1.8256350755691528\n",
            "\n",
            "Epoch [100/1000], Loss: 222.0293\n",
            "Epoch [200/1000], Loss: 11.6723\n",
            "Epoch [300/1000], Loss: 1.9166\n",
            "Epoch [400/1000], Loss: 1.3790\n",
            "Epoch [500/1000], Loss: 1.2307\n",
            "Epoch [600/1000], Loss: 1.1713\n",
            "Epoch [700/1000], Loss: 1.1291\n",
            "Epoch [800/1000], Loss: 1.1173\n",
            "Epoch [900/1000], Loss: 1.0968\n",
            "Epoch [1000/1000], Loss: 1.0867\n",
            "Final loss on current layer:  1.0866777896881104\n",
            "\n",
            "Epoch [100/1000], Loss: 85.8400\n",
            "Epoch [200/1000], Loss: 2.9034\n",
            "Epoch [300/1000], Loss: 0.8762\n",
            "Epoch [400/1000], Loss: 0.7120\n",
            "Epoch [500/1000], Loss: 0.6645\n",
            "Epoch [600/1000], Loss: 0.6336\n",
            "Epoch [700/1000], Loss: 0.6231\n",
            "Epoch [800/1000], Loss: 0.6134\n",
            "Epoch [900/1000], Loss: 0.6009\n",
            "Epoch [1000/1000], Loss: 0.5907\n",
            "Final loss on current layer:  0.5906503200531006\n",
            "\n",
            "Epoch [100/1000], Loss: 24.6941\n",
            "Epoch [200/1000], Loss: 0.5200\n",
            "Epoch [300/1000], Loss: 0.3930\n",
            "Epoch [400/1000], Loss: 0.3591\n",
            "Epoch [500/1000], Loss: 0.3357\n",
            "Epoch [600/1000], Loss: 0.3215\n",
            "Epoch [700/1000], Loss: 0.3201\n",
            "Epoch [800/1000], Loss: 0.3149\n",
            "Epoch [900/1000], Loss: 0.3080\n",
            "Epoch [1000/1000], Loss: 0.3072\n",
            "Final loss on current layer:  0.3072272539138794\n",
            "\n",
            "Epoch [100/1000], Loss: 1.9963\n",
            "Epoch [200/1000], Loss: 0.2453\n",
            "Epoch [300/1000], Loss: 0.2012\n",
            "Epoch [400/1000], Loss: 0.1658\n",
            "Epoch [500/1000], Loss: 0.1085\n",
            "Epoch [600/1000], Loss: 0.1075\n",
            "Epoch [700/1000], Loss: 0.1423\n",
            "Epoch [800/1000], Loss: 0.1564\n",
            "Epoch [900/1000], Loss: 0.1322\n",
            "Epoch [1000/1000], Loss: 0.2366\n",
            "Final loss on current layer:  0.23658911883831024\n",
            "\n",
            "Mean frobenius value for current rank:  0.8093559116125106\n",
            "\n",
            "Norm values for rank  30 :\n",
            "Epoch [100/1000], Loss: 469.9415\n",
            "Epoch [200/1000], Loss: 5.2990\n",
            "Epoch [300/1000], Loss: 3.0416\n",
            "Epoch [400/1000], Loss: 2.7279\n",
            "Epoch [500/1000], Loss: 2.6014\n",
            "Epoch [600/1000], Loss: 2.5301\n",
            "Epoch [700/1000], Loss: 2.5098\n",
            "Epoch [800/1000], Loss: 2.4564\n",
            "Epoch [900/1000], Loss: 2.4614\n",
            "Epoch [1000/1000], Loss: 2.4056\n",
            "Final loss on current layer:  2.40557599067688\n",
            "\n",
            "Epoch [100/1000], Loss: 210.5543\n",
            "Epoch [200/1000], Loss: 6.2528\n",
            "Epoch [300/1000], Loss: 2.1597\n",
            "Epoch [400/1000], Loss: 1.8096\n",
            "Epoch [500/1000], Loss: 1.6356\n",
            "Epoch [600/1000], Loss: 1.5971\n",
            "Epoch [700/1000], Loss: 1.5544\n",
            "Epoch [800/1000], Loss: 1.5338\n",
            "Epoch [900/1000], Loss: 1.5215\n",
            "Epoch [1000/1000], Loss: 1.5001\n",
            "Final loss on current layer:  1.5000759363174438\n",
            "\n",
            "Epoch [100/1000], Loss: 80.6131\n",
            "Epoch [200/1000], Loss: 3.3346\n",
            "Epoch [300/1000], Loss: 1.2719\n",
            "Epoch [400/1000], Loss: 0.9926\n",
            "Epoch [500/1000], Loss: 0.9062\n",
            "Epoch [600/1000], Loss: 0.8505\n",
            "Epoch [700/1000], Loss: 0.8277\n",
            "Epoch [800/1000], Loss: 0.7863\n",
            "Epoch [900/1000], Loss: 0.7695\n",
            "Epoch [1000/1000], Loss: 0.7560\n",
            "Final loss on current layer:  0.755951464176178\n",
            "\n",
            "Epoch [100/1000], Loss: 20.0813\n",
            "Epoch [200/1000], Loss: 0.8425\n",
            "Epoch [300/1000], Loss: 0.5270\n",
            "Epoch [400/1000], Loss: 0.4883\n",
            "Epoch [500/1000], Loss: 0.4618\n",
            "Epoch [600/1000], Loss: 0.4500\n",
            "Epoch [700/1000], Loss: 0.4395\n",
            "Epoch [800/1000], Loss: 0.4320\n",
            "Epoch [900/1000], Loss: 0.4280\n",
            "Epoch [1000/1000], Loss: 0.4231\n",
            "Final loss on current layer:  0.4230535924434662\n",
            "\n",
            "Epoch [100/1000], Loss: 1.5493\n",
            "Epoch [200/1000], Loss: 0.3818\n",
            "Epoch [300/1000], Loss: 0.2304\n",
            "Epoch [400/1000], Loss: 0.1850\n",
            "Epoch [500/1000], Loss: 0.1861\n",
            "Epoch [600/1000], Loss: 0.1467\n",
            "Epoch [700/1000], Loss: 0.1628\n",
            "Epoch [800/1000], Loss: 0.4374\n",
            "Epoch [900/1000], Loss: 0.1646\n",
            "Epoch [1000/1000], Loss: 0.2846\n",
            "Final loss on current layer:  0.28464174270629883\n",
            "\n",
            "Mean frobenius value for current rank:  1.0738597452640533\n",
            "\n",
            "Layer: fc1.weight, Rank: 30\n",
            "Layer: fc2.weight, Rank: 30\n",
            "Layer: fc3.weight, Rank: 30\n",
            "Layer: fc4.weight, Rank: 30\n",
            "Layer: fc5.weight, Rank: 10\n",
            "Average Rank of Matrices (Low-Rank Approximated Model): 26.00\n"
          ]
        }
      ],
      "source": [
        "rank_values = [5, 10, 20, 30]\n",
        "apply_low_rank_approximation(large_model, rank_values)\n",
        "\n",
        "low_rank_model_ranks = calculate_ranks(large_model)\n",
        "\n",
        "#average rank of low-rank approximated model\n",
        "low_rank_average_rank = np.mean(low_rank_model_ranks)\n",
        "print(f'Average Rank of Matrices (Low-Rank Approximated Model): {low_rank_average_rank:.2f}')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The smallest mean loss (frobenius norm) is for rank = 20, so we'll further use it as the effective one"
      ],
      "metadata": {
        "id": "2zOkxibnxctu"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PrBwf5bwPOny"
      },
      "source": [
        "## Task 6: Learning with low-rank factorization\n",
        "\n",
        "Once you found the effective rank $r$, take the same architecture from the previous task, and now replace each layer $A \\in \\mathbb{R}^{n\\times m}$ by a layer that applies $B\\cdot C$ with $B\\in \\mathbb{R}^{n\\times r}$ and $C \\in \\mathbb{R}^{r\\times m}$.\n",
        "\n",
        "**Question**: How much memory do you save? (you can just count the number of parameters of the original network and compare to that of the new network).\n",
        "\n",
        "Initialize these values with standard initialization, and train this network.\n",
        "\n",
        "**Question**: How does the learning change? Does it converge faster or slower? What about accuracy on both training and testing sets?\n",
        "\n",
        "**Question**: Now try doing inference, how much improvement do you see?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "lsh6iPILxbs9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T_oFw7TlRHFs"
      },
      "outputs": [],
      "source": [
        "class approximated_model(nn.Module):\n",
        "    def __init__(self, rank):\n",
        "        super().__init__()\n",
        "        self.rank = rank\n",
        "        self.activation = nn.ReLU()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.B1 = nn.Linear(self.rank, 1024, bias=False)\n",
        "        self.C1 = nn.Linear(784, self.rank, bias=False)\n",
        "        self.B2 = nn.Linear(self.rank, 1024, bias=False)\n",
        "        self.C2 = nn.Linear(1024, self.rank, bias=False)\n",
        "        self.B3 = nn.Linear(self.rank, 512, bias=False)\n",
        "        self.C3 = nn.Linear(1024, self.rank, bias=False)\n",
        "        self.B4 = nn.Linear(self.rank, 256, bias=False)\n",
        "        self.C4 = nn.Linear(512, self.rank, bias=False)\n",
        "        self.A5 = nn.Linear(256, 10, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        a1 = self.activation(self.B1(self.C1(x)))\n",
        "\n",
        "        a2 = self.activation(self.B2(self.C2(a1)))\n",
        "        a3 = self.activation(self.B3(self.C3(a2)))\n",
        "        a4 = self.activation(self.B4(self.C4(a3)))\n",
        "\n",
        "        logits = self.A5(a4)\n",
        "        return logits\n",
        "\n",
        "model_approximated = approximated_model(20).to(device)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#calculating memory savings\n",
        "def count_parameters(model):\n",
        "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "original_params = count_parameters(large_model)\n",
        "modified_params = count_parameters(model_approximated)\n",
        "\n",
        "memory_savings = original_params - modified_params\n",
        "print(f\"Memory savings: {memory_savings} parameters\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uPmMlmEIBXQ1",
        "outputId": "50e5557f-96e7-422c-babc-5bc3909fe983"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Memory savings: 449290 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model_approximated.parameters(), lr=0.001)\n",
        "\n",
        "train_losses = []\n",
        "train_accuracies = []\n",
        "num_epochs = 10\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    model_approximated.train()\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    for inputs, labels in train_loader:\n",
        "        inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model_approximated(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (torch.argmax(outputs, dim=1) == labels).sum().item()\n",
        "\n",
        "    train_loss = running_loss / len(train_loader)\n",
        "    train_acc = correct_train / total_train\n",
        "    train_losses.append(train_loss)\n",
        "    train_accuracies.append(train_acc)\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}],'f'\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2FgSfSiVBuV2",
        "outputId": "ec6f4ba9-a89e-4d2d-a435-74dcc3e62635"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10],\tTrain Loss: 0.654 | Train Acc: 78.01%\n",
            "Epoch [2/10],\tTrain Loss: 0.282 | Train Acc: 91.36%\n",
            "Epoch [3/10],\tTrain Loss: 0.222 | Train Acc: 93.16%\n",
            "Epoch [4/10],\tTrain Loss: 0.190 | Train Acc: 94.21%\n",
            "Epoch [5/10],\tTrain Loss: 0.175 | Train Acc: 94.60%\n",
            "Epoch [6/10],\tTrain Loss: 0.163 | Train Acc: 94.96%\n",
            "Epoch [7/10],\tTrain Loss: 0.150 | Train Acc: 95.37%\n",
            "Epoch [8/10],\tTrain Loss: 0.145 | Train Acc: 95.61%\n",
            "Epoch [9/10],\tTrain Loss: 0.133 | Train Acc: 96.00%\n",
            "Epoch [10/10],\tTrain Loss: 0.129 | Train Acc: 96.04%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tpYV1ebrkma1"
      },
      "source": [
        "## Task 7: Final conclusions\n",
        "\n",
        "Based on all the previous experiments, report your conclusions and try to give an explanation to the behaviours you observed.\n",
        "\n",
        "Can you think of other ways of using the low-rank factorizations? What about SVD? Provide an explanation."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "As we noticed from the first tasks, as we go deeper into the network, rank decreases. This decreasing rank pattern suggests that as we go deeper into the network, the weight matrices become less rank-deficient. It means that there is a reduction in the linearly independent columns in the weight matrices, indicating a compression and abstraction of information as it flows through the network.\n",
        "\n",
        "Therefore, in this specific MLP model, the ranks decrease as we go deeper into the network, indicating a decrease in the dimensionality or complexity of the learned representations.\n",
        "\n",
        "Another thing that can be mentioned is that even though we tried different MLP models ranks we got as a result are still equal to the dimensions of the following layers.\n",
        "\n",
        "Talking about the model in task 6, it is seen that the accuracy is a bit worse than the accuracy of large model that was created erlier, but is also pretty good. As for the memory savings, the original(large MLP) model has 449290 parameters more than the new approximated one, which alows to conclude that we save quite a good amount of memory using the new model."
      ],
      "metadata": {
        "id": "JLE0QB4N58ww"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JgH6pfFNQ196"
      },
      "source": [
        "## BONUS Task: LoRA\n",
        "\n",
        "Propose ideas by which low-rank could improve fine-tuning and training? Which disadvantages does it have?\n",
        "\n",
        "Read about LoRA (given in one of the references at the begining of the notebook).\n",
        "\n",
        "Now, take MNIST, and remove some digit from the dataset (keep the same labels, just remove the datapoints of a specific label).\n",
        "\n",
        "Train a simple MLP on this modified dataset.\n",
        "Fine-tune in the datapoints of the chosen digit, by using LoRA.\n",
        "\n",
        "Report the memory and time overheads."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSJgfNlOMGxa"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}